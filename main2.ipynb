{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c950d073-c852-48b2-b9b1-0db7bac78c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d02bb358-4971-4bd6-8d67-de24b278e09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68ca1783-d021-4d9d-8c10-6dc9a50572ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16544253-1734-4fa5-a12f-f5950e919381",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharTokenizer:\n",
    "    def __init__(self, extra_chars=None):\n",
    "        self.extra_chars = extra_chars or []\n",
    "        self.pad_token = '<PAD>'\n",
    "        self.unk_token = '<UNK>'\n",
    "        self.pad_id = 0\n",
    "        self.unk_id = 1\n",
    "        self.char2id = {self.pad_token: self.pad_id, self.unk_token: self.unk_id}\n",
    "        self.id2char = {self.pad_id: self.pad_token, self.unk_id: self.unk_token}\n",
    "        self.next_id = 2\n",
    "\n",
    "    def build_vocab(self, texts, min_freq=1):\n",
    "        cnt = Counter()\n",
    "        for t in texts:\n",
    "            for ch in t:\n",
    "                cnt[ch] += 1\n",
    "        for ch, c in cnt.items():\n",
    "            if c >= min_freq and ch not in self.char2id:\n",
    "                self.char2id[ch] = self.next_id\n",
    "                self.id2char[self.next_id] = ch\n",
    "                self.next_id += 1\n",
    "        for ch in self.extra_chars:\n",
    "            if ch not in self.char2id:\n",
    "                self.char2id[ch] = self.next_id\n",
    "                self.id2char[self.next_id] = ch\n",
    "                self.next_id += 1\n",
    "\n",
    "    def encode(self, text):\n",
    "        return [self.char2id.get(ch, self.unk_id) for ch in text]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return ''.join(self.id2char.get(i, '?') for i in ids)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.char2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd34cbba-52f7-44a4-811d-030aa2b1bede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boundaries_from_text_with_spaces(text):\n",
    "    positions = []\n",
    "    idx = 0\n",
    "    for ch in text:\n",
    "        if ch == ' ':\n",
    "            positions.append(idx)\n",
    "        else:\n",
    "            idx += 1\n",
    "    return positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ed967d2-795d-459b-9748-e6a6b7b01d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_from_text_without_spaces(orig_with_spaces):\n",
    "    s = orig_with_spaces\n",
    "    positions = boundaries_from_text_with_spaces(s)\n",
    "    compact = s.replace(' ', '')\n",
    "    n = len(compact)\n",
    "    labels = [0] * n\n",
    "    for p in positions:\n",
    "        if p == 0:\n",
    "            continue\n",
    "        labels[p-1] = 1\n",
    "    return compact, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43c2d9d5-6f6a-4e60-aab8-447afea0bcf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "книга в хорошем состоянии\n",
      "книгавхорошемсостоянии\n",
      "[0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "s = 'книга в хорошем состоянии'\n",
    "compact, labels = labels_from_text_without_spaces(s)\n",
    "print(s)\n",
    "print(compact)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f2f54b8-b774-411a-99e9-fcfc914752cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesize_from_corpus(lines, remove_prob=1.0, keep_word_prob=0.0):\n",
    "    out = []\n",
    "    for ln in lines:\n",
    "        ln = ln.strip()\n",
    "        if not ln:\n",
    "            continue\n",
    "        if random.random() < remove_prob:\n",
    "            compact = ln.replace(' ', '')\n",
    "            compact2, labels = labels_from_text_without_spaces(ln)\n",
    "            out.append((compact2, labels))\n",
    "        else:\n",
    "            parts = ln.split()\n",
    "            if len(parts) == 1:\n",
    "                compact2, labels = labels_from_text_without_spaces(ln)\n",
    "                out.append((compact2, labels))\n",
    "                continue\n",
    "            new_s = parts[0]\n",
    "            labels_positions = []\n",
    "            for i in range(1, len(parts)):\n",
    "                if random.random() < keep_word_prob:\n",
    "                    new_s += ' ' + parts[i]\n",
    "                else:\n",
    "                    new_s += parts[i]\n",
    "            compact2, labels = labels_from_text_without_spaces(new_s)\n",
    "            out.append((compact2, labels))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35757a49-b400-4f68-920f-b84c9832fc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoundaryDataset(Dataset):\n",
    "    def __init__(self, samples, tokenizer, max_len=256):\n",
    "        self.samples = samples\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        s, labels = self.samples[idx]\n",
    "        ids = self.tokenizer.encode(s)\n",
    "        if len(ids) > self.max_len:\n",
    "            ids = ids[:self.max_len]\n",
    "            labels = labels[:self.max_len]\n",
    "        return torch.tensor(ids, dtype=torch.long), torch.tensor(labels, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe3d4ce2-8eb3-4c30-858a-7f5bc374a176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    ids_list, labels_list = zip(*batch)\n",
    "    lengths = [len(x) for x in ids_list]\n",
    "    max_len = max(lengths)\n",
    "    padded_ids = torch.zeros(len(batch), max_len, dtype=torch.long)\n",
    "    padded_labels = torch.zeros(len(batch), max_len, dtype=torch.float)\n",
    "    mask = torch.zeros(len(batch), max_len, dtype=torch.bool)\n",
    "    for i, (ids, labs) in enumerate(zip(ids_list, labels_list)):\n",
    "        L = len(ids)\n",
    "        padded_ids[i, :L] = ids\n",
    "        padded_labels[i, :L] = labs\n",
    "        mask[i, :L] = 1\n",
    "    return padded_ids, padded_labels, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf5c8841-17e3-4ffa-8bcd-07d1daea9d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d804b34-a66c-4ad1-8313-71cc212f72d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightweightTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=128, nhead=4, num_layers=4, dim_feedforward=256, max_len=256, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
    "        self.pos_enc = PositionalEncoding(d_model, max_len=max_len)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.out = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, ids, src_key_padding_mask=None):\n",
    "        x = self.token_emb(ids)\n",
    "        x = self.pos_enc(x)\n",
    "        x = self.transformer(x, src_key_padding_mask=src_key_padding_mask)\n",
    "        logits = self.out(x).squeeze(-1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d3a2e31-ef86-4b91-8ca6-90c304a7aa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "233b21bc-343b-4665-bbb5-32a530572fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bce_loss_fn = nn.BCEWithLogitsLoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e55cb18-dee4-44a0-b604-0fb7171b30c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_masked_loss(logits, labels, mask, pos_weight=1.0):\n",
    "    loss = bce_loss_fn(logits, labels)\n",
    "    if pos_weight != 1.0:\n",
    "        pos_mask = (labels == 1.0).float()\n",
    "        loss = loss * (1.0 + (pos_weight - 1.0) * pos_mask)\n",
    "    loss = loss * mask.float()\n",
    "    return loss.sum() / (mask.float().sum() + 1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9cdbdc2-1dee-460f-95cb-8325dc8d2c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positions_from_logits(logits_row, mask_row, threshold=0.5):\n",
    "    probs = torch.sigmoid(logits_row)\n",
    "    assert probs.shape == mask_row.shape\n",
    "    preds = (probs >= threshold) & (mask_row.bool())\n",
    "    return [int(i) for i, v in enumerate(preds.tolist()) if v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a8ac9d9-af77-490f-ae3d-e6fea48b461a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_batch_from_logits(logits, labels, mask, threshold=0.5):\n",
    "    batch = logits.size(0)\n",
    "    f1s = []\n",
    "    for i in range(batch):\n",
    "        logits_row = logits[i]\n",
    "        labels_row = labels[i]\n",
    "        mask_row = mask[i]\n",
    "        pred_pos = positions_from_logits(logits_row, mask_row, threshold)\n",
    "        true_pos = [int(j) for j in range(labels_row.shape[0]) if mask_row[j] and float(labels_row[j]) >= 0.5]\n",
    "        f1s.append(_f1_single(pred_pos, true_pos))\n",
    "    return sum(f1s) / len(f1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f2f433b-4611-4fa6-8c36-56430e4e947e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _f1_single(pred, true):\n",
    "    if not pred and not true:\n",
    "        return 1.0\n",
    "    if not pred:\n",
    "        return 0.0\n",
    "    if not true:\n",
    "        return 0.0\n",
    "    pred_set = set(pred)\n",
    "    true_set = set(true)\n",
    "    tp = len(pred_set & true_set)\n",
    "    prec = tp / len(pred_set) if len(pred_set) > 0 else 0.0\n",
    "    rec = tp / len(true_set) if len(true_set) > 0 else 0.0\n",
    "    if prec + rec == 0:\n",
    "        return 0.0\n",
    "    return 2 * (prec * rec) / (prec + rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11d8013a-73b0-4791-8ec8-67fea526e475",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader=None, epochs=5, lr=2e-4, pos_weight=3.0, grad_clip=1.0, model_path='model.pt'):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    best_val = -1.0\n",
    "    model.to(DEVICE)\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch}\")\n",
    "        for ids, labels, mask in pbar:\n",
    "            ids = ids.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            mask = mask.to(DEVICE)\n",
    "            src_key_padding_mask = ~mask  # transformer expects True for pads\n",
    "            logits = model(ids, src_key_padding_mask=src_key_padding_mask)\n",
    "            loss = compute_masked_loss(logits, labels, mask, pos_weight=pos_weight)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            optimizer.step()\n",
    "            total_loss += float(loss.item())\n",
    "            pbar.set_postfix(loss=total_loss / (pbar.n + 1))\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch} avg_loss={avg_loss:.4f}\")\n",
    "\n",
    "        if val_loader is not None:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_f1 = 0.0\n",
    "                count = 0\n",
    "                for ids, labels, mask in val_loader:\n",
    "                    ids = ids.to(DEVICE)\n",
    "                    labels = labels.to(DEVICE)\n",
    "                    mask = mask.to(DEVICE)\n",
    "                    src_key_padding_mask = ~mask\n",
    "                    logits = model(ids, src_key_padding_mask=src_key_padding_mask)\n",
    "                    f1 = f1_batch_from_logits(logits.cpu(), labels.cpu(), mask.cpu(), threshold=0.5)\n",
    "                    val_f1 += f1\n",
    "                    count += 1\n",
    "                val_f1 = val_f1 / count if count > 0 else 0.0\n",
    "            print(f\"Validation F1: {val_f1:.4f}\")\n",
    "            if val_f1 > best_val:\n",
    "                best_val = val_f1\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "                print(f\"Saved best model with F1={best_val:.4f} to {model_path}\")\n",
    "    if os.path.exists(model_path):\n",
    "        model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da2b2140-453f-4e92-978d-dd65a2fa2b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 32\n",
    "EMBED_DIM = 128\n",
    "NHEAD = 4\n",
    "NUM_LAYERS = 4\n",
    "FF_DIM = 256\n",
    "EPOCHS = 1\n",
    "LR = 2e-4\n",
    "POS_WEIGHT = 3.0\n",
    "MODEL_PATH = 'light_transformer.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93896c8e-1165-411a-843f-85524802a410",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS_PATH = 'corpus.txt'\n",
    "TASK_PATH = 'task_data.txt'\n",
    "OUT_SUBMISSION = 'submission.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d4ebd24e-738f-4ed0-afd9-6cbea83b1838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сохранено 457 строк в corpus.txt\n"
     ]
    }
   ],
   "source": [
    "import re, random\n",
    "from pathlib import Path\n",
    "\n",
    "TRAIN_CSV = 'train.csv'\n",
    "OUT_CORPUS = 'corpus.txt'\n",
    "\n",
    "df = pd.read_csv(TRAIN_CSV, usecols=lambda c: c in ('title','description'), dtype=str)\n",
    "# Из-за ограничений по времени возьму только малый сабсет выборки, это сильно повлияет на качество модели, но моя цель показать\n",
    "# идею подхода\n",
    "df = df[:1000]\n",
    "df = df.fillna('')\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    s = str(s)\n",
    "    s = re.sub(r'<[^>]+>', ' ', s)\n",
    "    s = re.sub(r'http\\\\S+|www\\\\.\\\\S+', ' ', s)\n",
    "    s = re.sub(r'\\\\s+', ' ', s)\n",
    "    return s.strip()\n",
    "\n",
    "combined = (df['title'].fillna('') + ' ' + df['description'].fillna('')).map(clean_text)\n",
    "\n",
    "def is_russian_line(s: str) -> bool:\n",
    "    return bool(re.search('[а-яА-ЯёЁ]', s))\n",
    "\n",
    "keywords = ['продам','куплю','продаю','обмен','новый','б/у','торг','продажа','продается','срочно']\n",
    "\n",
    "out_lines = []\n",
    "for s in combined:\n",
    "    if not isinstance(s, str) or len(s) < 3:\n",
    "        continue\n",
    "    if not is_russian_line(s):\n",
    "        continue\n",
    "    if len(s) > 800:\n",
    "        s = s[:800]\n",
    "    low = s.lower()\n",
    "    if any(k in low for k in keywords) or (20 <= len(low) <= 200 and random.random() < 0.03):\n",
    "        out_lines.append(s)\n",
    "\n",
    "seen = set()\n",
    "corpus_lines = []\n",
    "for ln in out_lines:\n",
    "    if ln in seen:\n",
    "        continue\n",
    "    seen.add(ln)\n",
    "    corpus_lines.append(ln)\n",
    "\n",
    "with open(OUT_CORPUS, 'w', encoding='utf-8') as f:\n",
    "    for ln in corpus_lines:\n",
    "        f.write(ln + '\\n')\n",
    "\n",
    "print(\"Сохранено\", len(corpus_lines), \"строк в\", OUT_CORPUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0053eb28-ab9d-4eae-99fd-da53eec3b4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if Path(CORPUS_PATH).exists():\n",
    "    with open(CORPUS_PATH, 'r', encoding='utf-8') as f:\n",
    "        corpus_lines = [ln.strip() for ln in f if ln.strip()]\n",
    "else:\n",
    "    corpus_lines = [\n",
    "        'книга в хорошем состоянии',\n",
    "        'продам айфон 11 в отличном состоянии',\n",
    "        'новая кофеварка недорого',\n",
    "        'велосипед для детей 12 дюймов',\n",
    "        'работа удаленно программист',\n",
    "        'куплю часы брендовые',\n",
    "        'посудомоечная машина б/у',\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "43fde7bc-962b-4308-b7c1-f49cbb97850f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = synthesize_from_corpus(corpus_lines * 200, remove_prob=1.0)  # amplify\n",
    "random.shuffle(train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ab6cac55-6608-488d-bf98-27b1dd17952d",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(0.9 * len(train_samples))\n",
    "train_samples_list = train_samples[:split]\n",
    "val_samples_list = train_samples[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b70b47d5-4712-4e14-af6b-00f268032b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CharTokenizer()\n",
    "all_texts = [s for s, _ in train_samples_list] + [s for s, _ in val_samples_list]\n",
    "all_texts += [ln.replace(' ', '') for ln in corpus_lines]\n",
    "tokenizer.build_vocab(all_texts, min_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2b14da5b-9177-4a52-963f-ca8e35401c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = BoundaryDataset(train_samples_list, tokenizer, max_len=MAX_LEN)\n",
    "val_ds = BoundaryDataset(val_samples_list, tokenizer, max_len=MAX_LEN)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9bbb6dc5-5845-492e-a6b5-c8cb8a3a044e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightweightTransformer(\n",
      "  (token_emb): Embedding(174, 128, padding_idx=0)\n",
      "  (pos_enc): PositionalEncoding()\n",
      "  (transformer): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-3): 4 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (out): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = LightweightTransformer(vocab_size=len(tokenizer), d_model=EMBED_DIM, nhead=NHEAD, num_layers=NUM_LAYERS, dim_feedforward=FF_DIM, max_len=MAX_LEN)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "14dd5e95-e145-4741-b035-8c80621061c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|█████████████████████████████████████████████████████████| 6610/6610 [03:00<00:00, 36.60it/s, loss=0.278]\n",
      "C:\\Users\\user\\anaconda3\\envs\\sd_sampling\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:508: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\NestedTensorImpl.cpp:182.)\n",
      "  output = torch._nested_tensor_from_mask(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_loss=0.2780\n",
      "Validation F1: 0.9886\n",
      "Saved best model with F1=0.9886 to light_transformer.pt\n"
     ]
    }
   ],
   "source": [
    "model = train_model(model, train_loader, val_loader=val_loader, epochs=EPOCHS, lr=LR, pos_weight=POS_WEIGHT, model_path=MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8173bd61-b89d-4d65-b209-98cb31a5827c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "with open(\"task_data.txt\", encoding=\"utf-8\") as f:\n",
    "    header = next(f).rstrip(\"\\n\")\n",
    "\n",
    "    for lineno, line in enumerate(f, start=2):\n",
    "        line = line.rstrip(\"\\n\")\n",
    "        if not line:\n",
    "            continue\n",
    "        parts = line.split(\",\", 1)\n",
    "        if len(parts) == 2:\n",
    "            rows.append(parts)\n",
    "        else:\n",
    "            rows.append([parts[0], \"\"])\n",
    "\n",
    "df = pd.DataFrame(rows, columns=[\"id\", \"text\"])\n",
    "texts = df['text'].astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "851aca8b-c202-42fc-a9d7-456242e4574f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_positions_for_texts(texts, model, tokenizer, max_len=256, threshold=0.5):\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for t in texts:\n",
    "            s = t.strip()\n",
    "            ids = tokenizer.encode(s)\n",
    "            if len(ids) > max_len:\n",
    "                ids = ids[:max_len]\n",
    "            ids_t = torch.tensor([ids], dtype=torch.long).to(DEVICE)\n",
    "            mask = torch.ones(1, ids_t.size(1), dtype=torch.bool).to(DEVICE)\n",
    "            src_key_padding_mask = ~mask\n",
    "            logits = model(ids_t, src_key_padding_mask=src_key_padding_mask)\n",
    "            logits = logits[0].cpu()\n",
    "            pos = positions_from_logits(logits, torch.ones_like(logits, dtype=torch.bool), threshold=threshold)\n",
    "            preds.append(pos)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "82737b3d-8dbe-405b-b22d-d0e646b5d5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preds example: [[4, 6, 9], [2, 5, 10], [5, 6, 11, 28], [4, 12, 13], [4]]\n"
     ]
    }
   ],
   "source": [
    "preds = infer_positions_for_texts(texts, model, tokenizer, max_len=MAX_LEN, threshold=0.5)\n",
    "print('Preds example:', preds[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cece0a03-b11c-48d4-ac56-3ccf55154fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved submission to submission2.csv\n"
     ]
    }
   ],
   "source": [
    "df['predicted_positions'] = [str(p) for p in preds]\n",
    "df = df.drop('text', axis=1)\n",
    "OUT = 'submission.csv'\n",
    "df.to_csv(OUT, index=False)\n",
    "print('Saved submission to', OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8d95f291-ace7-472a-91fd-5996ee73ad1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_spaces(text, spaces):\n",
    "    spaces = set(spaces)\n",
    "    _str = []\n",
    "    for i, c in enumerate(text):\n",
    "        _str.append(c)\n",
    "        if i in spaces:\n",
    "            _str.append(\" \")\n",
    "    res_str = \"\".join(_str)\n",
    "    return res_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e07efaf-e713-4c67-9461-178063a88609",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
